# Микросервисное приложение "Напоминалка": от идеи до развертывания в Kubernetes

### Глава 1: Введение и архитектура проекта

Эта глава закладывает концептуальный фундамент для понимания всего проекта. Мы детально разберем, из каких частей состоит приложение, почему была выбрана именно такая архитектура, и за что отвечает каждый файл и каждая папка в репозитории.

#### 1.1. Описание проекта

Данный репозиторий представляет собой полнофункциональное веб-приложение "Напоминалка" и всю необходимую инфраструктуру для его развертывания в облаке. Приложение позволяет пользователям управлять текстовыми заметками-напоминаниями, задавая для них временные интервалы. Поддерживаются все CRUD-операции: создание, чтение, обновление и удаление напоминаний.

Проект является комплексной демонстрацией современных DevOps-практик и охватывает три ключевых этапа (или "слоя") развертывания, каждый из которых сложнее предыдущего:

1.  **Слой 1: Непрерывная интеграция (CI) и Тестирование.** На этом уровне мы фокусируемся на качестве и корректности кода самого приложения. Мы настраиваем автоматическую систему, которая проверяет каждое изменение в коде, запуская набор тестов. Это гарантирует, что новые доработки не ломают существующий функционал.

2.  **Слой 2: Контейнеризация и базовое облачное развертывание.** Здесь мы "упаковываем" наше приложение в Docker-контейнеры и разворачиваем его на одной виртуальной машине в облаке VK Cloud, делая его доступным из любой точки мира по публичному IP-адресу. Этот этап демонстрирует основы работы с облачной инфраструктурой и контейнерами.

3.  **Слой 3: Оркестрация, масштабируемость и мониторинг.** Это самый продвинутый уровень, на котором мы переходим от одной виртуальной машины к полноценному, отказоустойчивому **кластеру Kubernetes**. Мы настраиваем автоматическое масштабирование приложения в зависимости от нагрузки и внедряем систему мониторинга для отслеживания состояния системы в реальном времени.

Этот `README.md` служит подробным пошаговым руководством по прохождению всех трех этапов.

#### 1.2. Концепция и выбранная архитектура: Микросервисы

Изначально стояла задача создать веб-приложение. Вместо создания одного большого монолитного приложения, в котором и логика, и отображение, и работа с базой данных находятся в одном месте, был выбран **микросервисный подход**. Это означает, что вся система была разделена на несколько небольших, независимых и слабо связанных друг с другом сервисов.

В нашем проекте система состоит из **трех основных микросервисов**:

1.  **Frontend-сервис**: Это "лицо" нашего приложения, с которым напрямую взаимодействует пользователь через браузер. Его единственная задача — предоставлять веб-интерфейс. Он написан на Python с использованием фреймворка **FastAPI**, но его основная работа — это не вычисления, а генерация HTML-страниц с помощью шаблонизатора **Jinja2**. Он **не знает**, как и где хранятся данные. Когда ему нужна информация (например, список напоминаний), он обращается по сети к `Backend`-сервису.

2.  **Backend-сервис**: Это "мозг" и "сердце" приложения. Он также написан на FastAPI, но, в отличие от фронтенда, не работает с HTML. Его задача — предоставлять программный интерфейс (**REST API**) для работы с данными. Он получает запросы (например, "создай новое напоминание") от `Frontend`-сервиса, выполняет всю бизнес-логику и взаимодействует с базой данных для сохранения, изменения или извлечения информации. Он общается с другими сервисами, используя универсальный формат данных **JSON**.

3.  **База данных (PostgreSQL)**: Это специализированный сервис для надежного хранения данных. В нашем проекте мы используем **PostgreSQL** — мощную, популярную и проверенную временем реляционную систему управления базами данных. Она не содержит никакой бизнес-логики, а лишь выполняет команды от `Backend`-сервиса по сохранению, изменению и выдаче данных.

**Преимущества такой архитектуры:**
*   **Независимость**: Каждый сервис можно разрабатывать, тестировать, обновлять и масштабировать независимо от других. Если нам нужно улучшить только логику работы с БД, мы меняем только `Backend`.
*   **Отказоустойчивость**: Сбой в одном сервисе (например, `Frontend`) не обязательно приведет к полному отказу всей системы — `Backend` и база данных продолжат работать.
*   **Технологическая гибкость**: Хотя у нас оба сервиса на Python, в теории `Frontend` мог быть написан на Node.js, а `Backend` — на Go, и они бы все равно работали вместе, общаясь по стандартному протоколу HTTP с JSON.

#### 1.3. Детальная структура репозитория

Каждый файл и каждая папка в этом репозитории играют свою четкую роль.
```
reminder_app/
├── backend/                  # Директория с исходным кодом Backend-сервиса.
│   ├── __init__.py           # Пустой файл, указывающий Python, что 'backend' - это пакет.
│   ├── main.py               # Основной файл с API-эндпоинтами (`/api/reminders`), работающий с JSON.
│   ├── crud.py               # Функции для прямого взаимодействия с БД (Create, Read, Update, Delete).
│   ├── models.py             # Описание моделей данных: SQLAlchemy (для таблиц БД) и Pydantic (для валидации API).
│   └── database.py           # Настройка подключения к базе данных и функция инициализации таблиц.
│
├── frontend/                 # Директория с исходным кодом Frontend-сервиса.
│   ├── __init__.py           # Указывает Python, что 'frontend' - это пакет.
│   ├── main.py               # Основной файл, обрабатывающий веб-запросы от пользователя и рендерящий HTML.
│   └── templates/            # Папка с HTML-шаблонами, используемыми Jinja2.
│       ├── index.html        # Шаблон главной страницы.
│       └── edit_reminder.html# Шаблон страницы редактирования напоминания.
│
├── k8s/                      # Манифесты для развертывания в Kubernetes (Слой 3).
│   ├── backend.yaml          # Декларативное описание Deployment и Service для бэкенда.
│   ├── database.yaml         # Декларативное описание Deployment и Service для PostgreSQL.
│   ├── frontend.yaml         # Декларативное описание Deployment и Service для фронтенда.
│   └── hpa.yaml              # Описание HorizontalPodAutoscaler для автоматического масштабирования бэкенда.
│
├── helms/                    # Конфигурация для установки мониторинга через Helm
│   ├── prometheus/           # Распакованный чарт Prometheus
│   ├── grafana/              # Распакованный чарт Grafana
│   ├── trickster/            # Распакованный чарт Trickster
│   ├── grafana-values.yaml   # Файл с кастомными настройками для Grafana
│   ├── prometheus-values.yaml# Файл с кастомными настройками для Prometheus
|   ├── trickster-values.yaml # Файл с кастомными настройками для Trickster
│   └── grafana-ingress.yaml  # Это отдельный Kubernetes-манифест, который создает Ingress — специальный ресурс для
|                             # управления доступом к Grafana через веб. Он работает в связке с Ingress-Nginx,
|                             # который тоже устанавливается через Helm.
│
├── terraform/                # Конфигурация инфраструктуры как кода (IaC) для VK Cloud (на момент фнального слоя 3).
│   ├── main.tf               # Основной файл, описывающий создаваемые ресурсы (кластер K8s, ВМ, IP-адреса).
│   ├── network.tf            # Описание сетевой инфраструктуры (сеть, подсеть, маршрутизатор).
│   ├── variables.tf          # Объявление всех используемых переменных (имена, типы ВМ и т.д.).
│   ├── vkcs_provider.tf      # Настройки провайдера VK Cloud, указывающие Terraform, как подключаться к облаку.
|   ├── kubeconfig.yaml       # Файл с конфигурацией облака (скачиваем из UI на сайте VK Cloud в нашем проекте)
|   ├── .terraform.lock.hcl   # Файл "замка" версий (точные версии провайдеров для воспроизводимости на другом компьютере)
│   └── terraform.tfvars      # Локальный файл с вашими личными значениями переменных (пароли, ID). НЕ ДОБАВЛЯТЬ В GIT!
│
├── terraform-wo-k8s/          # Та же папка terraform/, но на момент слоя 2 (создание одной ВМ, без k8s/ вообще)
|
├── tests/                    # Модульные тесты для приложения.
│   ├── __init__.py           # Указывает Python, что 'tests' - это пакет.
│   └── test_main.py          # Файл с тестами, написанными с использованием pytest.
│
├── .github/                  # Конфигурация для GitHub.
│   └── workflows/            # Директория для описания CI/CD процессов.
│       └── ci.yml            # Workflow для GitHub Actions, отвечающий за непрерывную интеграцию (автозапуск тестов).
│
├── ddos.sh                   # Скрипт для нагрузочного тестирования приложения (с его помощью проверяем масштабирование)
|
├── Dockerfile.backend        # "Рецепт" (инструкция) для сборки Docker-образа Backend-сервиса.
├── Dockerfile.frontend       # "Рецепт" (инструкция) для сборки Docker-образа Frontend-сервиса.
│
├── docker-compose.yml        # Файл-оркестратор для локального запуска и тестирования всех трех сервисов вместе.
│
├── requirements_backend.txt  # Список Python-зависимостей, необходимых для работы бэкенда.
└── requirements_frontend.txt # Список Python-зависимостей, необходимых для работы фронтенда.
```

#### 1.4. Возможные проблемы и их решения (Этап локальной разработки)

В процессе разработки и настройки локального окружения возникает ряд типичных вопросов и проблем. Этот раздел служит сборником ответов и решений, с которыми мы столкнулись.

<details>
  <summary><strong>Вопрос: Что такое `uvicorn` и откуда берется база данных PostgreSQL?</strong></summary>

  > **Ответ:**
  > -   `uvicorn` — это **ASGI-сервер**. Фреймворк FastAPI, на котором написано приложение, определяет *логику* обработки запросов (что делать, когда пользователь заходит на `/` или отправляет данные), но ему нужен отдельный сервер, который будет физически принимать HTTP-запросы из сети и передавать их нашему приложению. Эту роль в нашем проекте выполняет `uvicorn`.
  > -   База данных PostgreSQL сама по себе не устанавливается из кода; наше приложение лишь **подключается** к уже запущенному серверу БД.
  >     -   При локальной разработке с `docker-compose`, мы используем готовый образ `postgres:14-alpine`, и `docker-compose` запускает его в отдельном контейнере.
  >     -   При развертывании в облаке это может быть управляемый сервис баз данных или, как в нашем случае, такой же Docker-контейнер, но запущенный уже в Kubernetes.

</details>

<details>
  <summary><strong>Вопрос: Зачем так много зависимостей в `requirements.txt`?</strong></summary>
  
  > **Ответ:** Каждая библиотека выполняет свою, строго определенную роль:
  > -   **`psycopg2-binary`**: Это Python-драйвер для подключения к PostgreSQL. SQLAlchemy (наша ORM для работы с БД) использует его "под капотом" для отправки SQL-запросов и получения результатов.
  > -   **`jinja2`**: Мощный шаблонизатор, который `Frontend`-сервис использует для генерации HTML-страниц путем вставки в них динамических данных (например, списка напоминаний).
  > -   **`python-dotenv`**: Утилита для удобства локальной разработки. Она позволяет загружать переменные окружения (например, `DATABASE_URL` с паролем от БД) из локального файла `.env`, который не хранится в системе контроля версий.
  > -   **`python-multipart`**: Необходима фреймворку FastAPI для корректной обработки данных, приходящих из HTML-форм (`multipart/form-data`). Без нее FastAPI не сможет распарсить данные, отправленные из наших форм создания и редактирования.

</details>

<details>
  <summary><strong>Вопрос: Есть ли проблема с использованием времени не в UTC?</strong></summary>
  
  > **Ответ:** Да, это важный архитектурный компромисс. **Лучшая практика** для любого глобального приложения — хранить все временные метки в базе данных в формате UTC (Coordinated Universal Time). Это позволяет избежать путаницы с часовыми поясами и переходом на летнее/зимнее время.
  >
  > Однако для упрощения разработки и в рамках учебного проекта было принято решение работать с "наивным" временем (`naive datetime`), которое берется из системного времени сервера. Это означает, что все временные метки считаются принадлежащими одному и тому же, неопределенному часовому поясу. Для реального продукта этот аспект потребовал бы обязательной доработки, например, путем конвертации времени пользователя в UTC перед сохранением в БД и обратной конвертации при отображении.

</details>

<details>
  <summary><strong>Вопрос: Локальный запуск `docker-compose up` падает с ошибкой `ImportError: attempted relative import with no known parent package`.</strong></summary>
  
  > **Ответ:** Эта ошибка возникала из-за того, что `uvicorn` запускался из директории (`/app`), которая не являлась Python-пакетом, и поэтому относительные импорты вида `from . import crud` не работали.
  >
  > **Решение:** Структура копирования в `Dockerfile` была изменена, чтобы сохранить структуру пакета внутри контейнера, и была скорректирована команда запуска:
  > -   **Было в `Dockerfile.backend`:** `COPY ./backend /app/` и `CMD ["uvicorn", "main:app", ...]`
  > -   **Стало:** `COPY ./backend /app/backend` и `CMD ["uvicorn", "backend.main:app", ...]`.
  > Теперь `uvicorn` запускает модуль `main` из пакета `backend`, что позволяет Python корректно разрешать относительные импорты.

</details>

<details>
  <summary><strong>Вопрос: При запуске тестов (`pytest tests/`) я вижу `ModuleNotFoundError: No module named 'backend'`.</strong></summary>
  
  > **Ответ:** `pytest` по умолчанию не знает о существовании папок `backend` и `frontend` на уровень выше.
  > -   **Решение 1 (правильное):** Запускайте просто `pytest` из корня проекта. В этом случае корневая директория автоматически добавляется в `PYTHONPATH`, и все импорты находятся.
  > -   **Решение 2 (использованное в проекте для простоты):** В начало `tests/test_main.py` был добавлен код, который вручную добавляет родительскую директорию в `sys.path`:
  >     ```python
  >     import sys
  >     import os
  >     sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
  >     ```
  > -   **Дополнительно:** Необходимо убедиться, что в каждой директории с кодом (`backend`, `frontend`, `tests`) есть пустой файл `__init__.py`.

</details>

<details>
  <summary><strong>Вопрос: Тесты падают с ошибкой `sqlalchemy.exc.OperationalError: no such table: reminders`.</strong></summary>
  
  > **Ответ:** Это означает, что тестовая база данных (in-memory SQLite или файловая) пуста и таблицы в ней не были созданы перед запуском теста.
  > -   **Решение:** В `tests/test_main.py` была добавлена `pytest`-фикстура `setup_db`, которая вызывает `Base.metadata.create_all(bind=engine)` перед каждым тестом и `Base.metadata.drop_all(bind=engine)` после него.
  > -   **Дополнительная проблема:** Даже с фикстурой ошибка могла сохраняться. Причина была в том, что метаданные SQLAlchemy были пусты в момент вызова `create_all`. **Решение:** В начало `tests/test_main.py` был добавлен явный импорт `from backend.models import Reminder`, чтобы модель успела зарегистрироваться в `Base.metadata` до вызова фикстуры.

</details>

<details>
  <summary><strong>Вопрос: Почему при нажатии на кнопку «Показать все» я получаю ошибку 422 Unprocessable Entity?</strong></summary>
  
  > **Ответ:** Это происходило потому, что `<form>` при отправке включала поля для ввода дат, которые были пустыми (`filter_start_date=`). FastAPI не мог преобразовать пустую строку в дату и возвращал ошибку валидации.
  > -   **Решение:** Логика в `index.html` была разделена. Вместо одной формы с двумя кнопками были сделаны две отдельные: одна — для фильтрации по датам, вторая, содержащая только одну кнопку, — для вывода полного списка.

</details>

<details>
  <summary><strong>Вопрос: Почему `Frontend`-сервис падал с ошибкой `jinja2.exceptions.UndefinedError: 'str object' has no attribute 'strftime'`?</strong></summary>

  > **Ответ:** Эта ошибка возникала после разделения на микросервисы. `Backend` возвращал данные в формате JSON, где все `datetime` объекты были преобразованы в обычные строки (например, `"2025-06-12T21:15:00"`). `Frontend` получал эти данные, передавал их в HTML-шаблон, где `Jinja2` пытался вызвать у этой **строки** метод `.strftime()`, которого у нее нет.
  >
  > **Решение:** В `frontend/main.py` была добавлена функция-помощник `parse_reminders`. Перед передачей данных в шаблон, эта функция "пробегается" по списку словарей от бэкенда и вручную конвертирует строковые представления дат обратно в полноценные объекты `datetime` с помощью `datetime.fromisoformat()`. Таким образом, в шаблон попадают уже правильные объекты, с которыми `strftime` может работать.

</details>






### Глава 2: Локальная разработка и Непрерывная Интеграция (CI)

На этом базовом этапе мы фокусируемся исключительно на качестве и корректности кода самого приложения. Наша цель — убедиться, что приложение работает как задумано на локальной машине, написать для него тесты и настроить автоматическую систему, которая будет проверять каждое изменение в коде. Этот этап не затрагивает облака, Docker или развертывание.

#### 2.1. Технологический стек и окружение

Для разработки и тестирования приложения на локальной машине используется следующий набор технологий:

*   **Язык и фреймворк:** Python 3.10 и веб-фреймворк FastAPI для создания обоих микросервисов (`frontend` и `backend`).
*   **Веб-сервер:** Uvicorn, высокопроизводительный ASGI-сервер, необходимый для запуска FastAPI-приложений.
*   **База данных (для локальной разработки):** На этом этапе, для максимального упрощения и отсутствия внешних зависимостей, приложение сконфигурировано для работы с **SQLite**. Это легковесная, файловая база данных, которая не требует установки отдельного сервера. Приложение автоматически создает файл `local_backend.db` (или у нас еще называется `./for_test.db`) в директории `backend/`, и вся база данных живёт локально в этом файле. Код написан так, чтобы при появлении переменной окружения `DATABASE_URL` (которая появится при запуске отдельного сервера с PostgreSQL), он автоматически переключался на полноценную PostgreSQL для последующих этапов (в этом случае файлик `local_backend.db` уже не нужен).
*   **Работа с БД из кода:** SQLAlchemy используется как ORM (Object-Relational Mapper) для удобного и безопасного взаимодействия с базой данных из Python-кода.
*   **Шаблонизатор:** Jinja2 применяется `frontend`-сервисом для генерации динамических HTML-страниц.
*   **Тестирование:** Pytest используется как фреймворк для написания и запуска модульных тестов, а `httpx` — для совершения тестовых HTTP-запросов к приложению.

Все необходимые Python-библиотеки перечислены в файлах `requirements_backend.txt` и `requirements_frontend.txt`.

#### 2.2. Запуск и проверка приложения локально

Для запуска всего приложения на одном компьютере (имитируя его работу, но без изоляции контейнеров) потребуется запустить три процесса в трех разных терминалах:

1.  **Запуск базы данных PostgreSQL (имитация)**: На этом этапе мы используем SQLite, поэтому запуск отдельного сервера БД не требуется. Приложение само создаст файл базы данных.
2.  **Запуск Backend-сервиса**:
    ```bash
    # Перейдите в корневую папку проекта
    vselenaya@computer:~/reminder_app$  python -m uvicorn backend.main:app --reload
    INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
    ...
    ```
3.  **Запуск Frontend-сервиса**:
    Поскольку порт `8000` уже занят бэкендом, запустим фронтенд на другом порту, например `8080`.
    ```bash
    # В новом окне терминала, из корневой папки проекта
    vselenaya@computer:~/reminder_app$ python -m uvicorn frontend.main:app --port 8080 --reload
    INFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)
    ...
    ```
    Приложение будет доступно в браузере по адресу `http://localhost:8080`.

> **Примечание:** Этот способ запуска используется только для демонстрации работы кода без Docker. На следующих этапах мы будем использовать `docker-compose`, который значительно упростит этот процесс.

#### 2.3. Модульное тестирование

Для обеспечения качества кода были написаны тесты с использованием `pytest`. Они находятся в директории `tests/` и проверяют ключевой функционал приложения.

**Выполнение тестов:**
Все тесты следует запускать из **корневой директории проекта** одной командой.
```bash
vselenaya@computer:~/reminder_app$ pytest
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.2.0, pluggy-1.5.0
rootdir: /home/vselenaya/reminder_app
collected 5 items

tests/test_main.py .....                                                 [100%]

============================== 5 passed in 1.50s ===============================
```
Этот вывод означает, что все 5 написанных тестов успешно пройдены.

#### 2.4. Настройка Непрерывной Интеграции (CI) с GitHub Actions

Цель Непрерывной Интеграции — автоматически проверять, что новые изменения в коде не сломали уже существующий функционал. Мы используем для этого встроенный инструмент **GitHub Actions**. Конфигурация этого процесса находится в файле `.github/workflows/ci.yml`.

**Как работает CI Workflow:**
-   **Триггер**: Процесс запускается автоматически на серверах GitHub при каждом `push` в ветку `main`.
-   **Задание `test` (Job)**:
    1.  GitHub выделяет чистую виртуальную машину с Ubuntu.
    2.  **Шаг "Checkout"**: На эту машину клонируется код из вашего репозитория.
    3.  **Шаг "Set up Python"**: Устанавливается указанная версия Python.
    4.  **Шаг "Install dependencies"**: Выполняется `pip install -r ...`, чтобы установить все зависимости проекта.
    5.  **Шаг "Run tests"**: Запускается команда `pytest`. Если хотя бы один тест проваливается, весь workflow останавливается и помечается как неудачный.
-   **Задание `build` (Job)**:
    > **Концепция:** В CI-пайплайнах часто разделяют этапы "тестирования" и "сборки". На этапе `test` мы проверяем логическую корректность кода. На этапе `build` мы проверяем, что из этого кода можно создать готовый к развертыванию артефакт (например, скомпилировать программу или, как в нашем случае, собрать Docker-образ).
    
    1.  Этот шаг запускается **только если задание `test` завершилось успешно** (определяется через `needs: test`).
    2.  Он выполняет те же подготовительные шаги, что и `test`.
    3.  В простом случае часто этот этап является символическим — он просто подтверждает, что после успешных тестов можно переходить к следующим шагам. В более сложном пайплайне здесь бы запускалась команда `docker build`.

Процесс выполнения CI можно отслеживать в реальном времени на вкладке "Actions" вашего репозитория на GitHub. Это обеспечивает постоянную обратную связь о качестве кода.


#### 2.5. Возможные проблемы и их решения (Этап локальной разработки и CI)

В процессе разработки и настройки локального окружения возникает ряд типичных вопросов и проблем. Этот раздел служит сборником ответов и решений, с которыми мы столкнулись, и объясняет, почему финальный код выглядит именно так.

<details>
  <summary><strong>Вопрос: Что такое `uvicorn` и откуда берется база данных PostgreSQL?</strong></summary>

  > **Ответ:**
  > -   `uvicorn` — это **ASGI-сервер**. Фреймворк FastAPI, на котором написано приложение, определяет *логику* обработки запросов (что делать, когда пользователь заходит на `/` или отправляет данные), но ему нужен отдельный сервер, который будет физически принимать HTTP-запросы из сети и передавать их нашему приложению. Эту роль в нашем проекте выполняет `uvicorn`.
  > -   База данных PostgreSQL сама по себе не устанавливается из кода; наше приложение лишь **подключается** к уже запущенному серверу БД.
  >     -   На этом этапе, для максимального упрощения и отсутствия внешних зависимостей, приложение по умолчанию работает с **SQLite**. Это легковесная, файловая база данных, которая не требует установки отдельного сервера.
  >     -   Приложение написано так, чтобы при появлении переменной окружения `DATABASE_URL` оно автоматически переключалось на PostgreSQL, что будет использоваться на следующих этапах.

</details>

<details>
  <summary><strong>Вопрос: Зачем так много зависимостей в `requirements.txt`?</strong></summary>
  
  > **Ответ:** Каждая библиотека выполняет свою, строго определенную роль:
  > -   **`psycopg2-binary`**: Это Python-драйвер для подключения к PostgreSQL. SQLAlchemy (наша ORM для работы с БД) использует его "под капотом" для отправки SQL-запросов и получения результатов. Он необходим для работы с PostgreSQL на следующих этапах.
  > -   **`jinja2`**: Мощный шаблонизатор, который `Frontend`-сервис использует для генерации HTML-страниц путем вставки в них динамических данных (например, списка напоминаний).
  > -   **`python-dotenv`**: Утилита для удобства локальной разработки. Она позволяет загружать переменные окружения (например, `DATABASE_URL` с паролем от БД) из локального файла `.env`, который не хранится в системе контроля версий.
  > -   **`python-multipart`**: Обязательная зависимость для FastAPI, необходимая для корректной обработки данных, приходящих из HTML-форм (`enctype="multipart/form-data"` или `application/x-www-form-urlencoded`). Без нее FastAPI не сможет распарсить данные, отправленные из наших форм создания и редактирования.

</details>

<details>
  <summary><strong>Вопрос: Есть ли проблема с использованием времени не в UTC?</strong></summary>
  
  > **Ответ:** Да, это важный архитектурный компромисс. **Лучшая практика** для любого глобального приложения — хранить все временные метки в базе данных в формате UTC (Coordinated Universal Time). Это позволяет избежать путаницы с часовыми поясами, переходом на летнее/зимнее время и корректно отображать время для пользователей из разных регионов.
  >
  > Однако для упрощения разработки и в рамках учебного проекта было принято решение работать с "наивным" временем (`naive datetime`), которое берется из системного времени сервера. Это означает, что все временные метки считаются принадлежащими одному и тому же, неопределенному часовому поясу. Для реального, продакшн-продукта этот аспект потребовал бы обязательной доработки.

</details>

<details>
  <summary><strong>Вопрос: При запуске тестов (`pytest tests/`) я вижу `ModuleNotFoundError: No module named 'backend'`.</strong></summary>
  
  > **Ответ:** Эта ошибка возникает потому, что `pytest`, будучи запущенным из директории `tests/`, не "видит" пакеты `backend` и `frontend`, которые находятся на уровень выше.
  > -   **Решение 1 (правильное и используемое в CI):** Запускайте команду `pytest` **без аргументов** из **корневой директории проекта**. В этом случае корневая директория (`reminder_app/`) автоматически добавляется в `PYTHONPATH`, и Python находит все дочерние пакеты.
  > -   **Решение 2 (использованное в проекте для удобства локальной отладки):** В начало `tests/test_main.py` был добавлен код, который вручную добавляет родительскую директорию в `sys.path`:
  >     ```python
  >     import sys
  >     import os
  >     sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
  >     ```
  > -   **Дополнительное, обязательное условие:** Убедитесь, что в каждой директории с кодом (`backend`, `frontend`, `tests`) есть пустой файл `__init__.py`. Он служит маркером для Python, что данная директория является пакетом.

</details>

<details>
  <summary><strong>Вопрос: Тесты падают с ошибкой `sqlalchemy.exc.OperationalError: no such table: reminders`.</strong></summary>
  
  > **Ответ:** Это означает, что тестовая база данных (в данном случае, файловая SQLite `./for_test.db`) пуста и таблицы в ней не были созданы перед запуском теста.
  > -   **Решение:** В `tests/test_main.py` была добавлена `pytest`-фикстура `setup_db`, которая выполняет `Base.metadata.create_all(bind=engine)` **перед** каждым тестом и `Base.metadata.drop_all(bind=engine)` **после** него. Это гарантирует, что каждый тест запускается в чистой базе данных.
  > -   **Применение фикстуры:** Чтобы фикстура применялась ко всем тестам в файле, была добавлена строка `pytestmark = pytest.mark.usefixtures("setup_db")`.
  > -   **Дополнительная проблема:** Даже с фикстурой ошибка могла сохраняться. Причина была в том, что метаданные SQLAlchemy (`Base.metadata`) были пусты в момент вызова `create_all`. **Решение:** В начало `tests/test_main.py` был добавлен явный импорт `from backend.models import Reminder`, чтобы модель успела зарегистрироваться в `Base.metadata` до вызова фикстуры.

</details>

<details>
  <summary><strong>Вопрос: Почему при нажатии на кнопку «Показать все» я получаю ошибку 422 Unprocessable Entity?</strong></summary>
  
  > **Ответ:** Это происходило потому, что `<form>` при отправке включала поля для ввода дат (`filter_start_date`, `filter_end_date`), которые были пустыми. FastAPI пытался преобразовать пустую строку `""` в тип `date`, что вызывало ошибку валидации.
  > -   **Решение:** Логика в `index.html` была разделена. Вместо одной формы с двумя кнопками были сделаны две отдельные: одна — для фильтрации по датам, вторая, содержащая только одну кнопку «Показать все», — для вывода полного списка. Это гарантирует, что при запросе всех напоминаний параметры дат вообще не отправляются на сервер.

</details>

<details>
  <summary><strong>Вопрос: Мой тест на создание/удаление падает или ведет себя непредсказуемо.</strong></summary>
  
  > **Ответ:** В процессе отладки тестов было выявлено несколько проблем:
  >   1.  **Неправильные эндпоинты**: Изначально тесты обращались к эндпоинтам API (`/api/reminders/`), которые возвращают JSON, в то время как нужно было тестировать эндпоинты для HTML-форм (`/reminders/`), которые ожидают данные в другом формате.
  >   2.  **Формат данных**: Имена полей в тестовом запросе не совпадали с атрибутами `name` в HTML-форме (например, `start_time` вместо `start_time_str`).
  >   3.  **Редиректы:** `TestClient` FastAPI по умолчанию **следует** за HTTP-редиректами (статус 3xx). Эндпоинт после успешной операции делает редирект на главную страницу, поэтому клиент получает финальную страницу со статусом 200 OK. Тесты были исправлены так, чтобы они ожидали статус 200 и проверяли наличие/отсутствие данных на **итоговой HTML-странице**, а не на промежуточном ответе с редиректом.
  >   4.  **Захардкоженный ID:** В одной из версий теста ID удаляемого напоминания был жестко задан как `1`. Это ненадежно, так как тесты могут запускаться в разном порядке. **Решение:** Вместо этого ID извлекается динамически из HTML-ответа после создания с помощью регулярного выражения: `re.search(r'ID: (\d+)', response.text)`.

</details>



### Глава 3: Слой 2 — Контейнеризация и локальная оркестрация

На этом этапе мы делаем важнейший шаг: "упаковываем" наши микросервисы в **Docker-контейнеры**. Это позволяет нам запускать все приложение целиком одной командой, в полностью изолированном и воспроизводимом окружении, а также подготавливает его к развертыванию в облаке.

#### 3.1. Концепции этапа

##### Что такое контейнеризация и зачем она нужна?

**Контейнеризация** — это процесс упаковки приложения и всех его зависимостей (библиотек, системных утилит, файлов конфигурации) в единый стандартизированный блок, называемый **контейнером**.

Представьте, что раньше для запуска приложения на новом сервере нужно было вручную устанавливать Python нужной версии, затем все библиотеки из `requirements.txt`, настраивать базу данных... Этот процесс был сложным и часто приводил к ошибкам ("а у меня на компьютере все работало!").

Контейнеры решают эту проблему. Контейнер — это как легковесная, портативная "коробка", внутри которой уже есть всё, что нужно нашему приложению для работы. Эту "коробку" можно запустить на любом компьютере, где установлен Docker, и она будет работать абсолютно одинаково.

##### Роль `Dockerfile.*`
`Dockerfile` (например, `Dockerfile.backend`) — это пошаговая инструкция, или "рецепт", для Docker по созданию **Docker-образа** — шаблона для наших будущих контейнеров. Образ — это, по сути, "слепок" файловой системы со всем необходимым.

Давайте разберем наш `Dockerfile.backend` по командам:
- `FROM python:3.10-slim`: Команда говорит Docker'у взять за основу готовый официальный образ с уже установленным Python 3.10. `slim` означает, что это облегченная версия.
- `WORKDIR /app`: Создать внутри будущего контейнера рабочую директорию `/app` и выполнять все последующие команды из нее.
- `COPY requirements_backend.txt .`: Скопировать с нашего локального компьютера файл с зависимостями в рабочую директорию `/app` внутри контейнера.
- `RUN pip install -r requirements_backend.txt`: Выполнить команду `pip install` **внутри контейнера** в процессе его сборки, чтобы установить все необходимые библиотеки.
- `COPY ./backend /app/backend`: Скопировать всю директорию с исходным кодом нашего бэкенда в поддиректорию `/app/backend` внутри контейнера.
- `CMD ["uvicorn", "backend.main:app", ...]`: Это команда, которая будет выполняться по умолчанию при **запуске** контейнера из созданного образа. Она запускает наш веб-сервер `uvicorn`.

##### Роль `docker-compose.yml`: оркестрация на одной машине

Если `Dockerfile` — это рецепт для одного "блюда" (микросервиса), то `docker-compose.yml` — это меню и план рассадки для целого "банкета" на **вашей локальной машине**. Он позволяет запустить и связать все наши три микросервиса одной командой.

Его ключевые функции в нашем проекте:
1.  **`services`**: Определяет три сервиса, которые составляют наше приложение: `frontend`, `backend` и `db`.
2.  **Инструкции по запуску**:
    -   Для `frontend` и `backend` он использует директиву `build:`, которая указывает Docker'у собрать для них образы локально, используя `Dockerfile.frontend` и `Dockerfile.backend`.
    -   Для `db` он использует директиву `image: postgres:14-alpine`, которая говорит Docker'у не собирать ничего, а просто скачать готовый официальный образ PostgreSQL из Docker Hub.
3.  **Создание общей сети (`networks`)**: `docker-compose` автоматически создает изолированную виртуальную сеть и подключает к ней все три контейнера. Это позволяет им общаться друг с другом по именам сервисов. Именно поэтому в коде `frontend` мы можем обратиться к бэкенду по адресу `http://backend:8000`, а `backend` — к базе данных по адресу `postgresql://myuser:mypassword@db:5432/reminders_db`. Имена `backend` и `db` разрешаются во внутренние IP-адреса контейнеров благодаря этой сети.
4.  **Хранение данных (`volumes`)**: Для сервиса `db` мы создаем `volume` с именем `postgres_data`. Это связывает папку внутри контейнера, где PostgreSQL хранит свои данные (`/var/lib/postgresql/data/`), со специальной областью на диске вашего компьютера, управляемой Docker. Благодаря этому, даже если вы остановите и удалите контейнер с базой данных, а потом запустите его снова, все ваши напоминания сохранятся.
5.  **Настройка окружения (`environment`)**: Он передает внутрь контейнеров переменные окружения, необходимые для их работы, например, `DATABASE_URL` для бэкенда или `POSTGRES_USER` и `POSTGRES_PASSWORD` для инициализации базы данных.
6.  **Проброс портов (`ports`)**: Директива `ports: - "80:8000"` у сервиса `frontend` "пробрасывает" порт из контейнера наружу. Она означает: "Все запросы, которые придут на порт `80` моего компьютера (`localhost`), перенаправляй на порт `8000` внутри контейнера `frontend`". Это позволяет нам заходить на сайт через браузер.

Таким образом, `docker-compose` полностью автоматизирует локальный запуск сложного многокомпонентного приложения.

#### 3.2. Запуск и отладка приложения локально с Docker Compose

Для комплексной проверки взаимодействия всех сервисов перед развертыванием в облако используется Docker Compose.

**Установка:**
Убедитесь, что на вашем компьютере установлен [Docker Desktop](https://www.docker.com/products/docker-desktop/) (для Windows/macOS) или Docker Engine + Docker Compose (для Linux).

**Запуск приложения:**
Все команды выполняются из корневой директории проекта, где лежит файл `docker-compose.yml`.

1.  **Сборка образов и первый запуск:**
    Эта команда соберет локальные образы для `frontend` и `backend`, скачает образ `PostgreSQL`, создаст сеть и том, и запустит все три контейнера. Логи всех контейнеров будут выводиться в ваш терминал.
    ```bash
    vselenaya@computer:~/reminder_app$ docker-compose up --build
    [+] Building 20.5s (24/24) FINISHED
    [+] Running 3/3
     ✔ Container reminder_db       Started
     ✔ Container reminder_backend  Started
     ✔ Container reminder_frontend Started
    Attaching to reminder_backend, reminder_db, reminder_frontend
    reminder_db       | The files belonging to this database system will be owned by user "postgres".
    ...
    reminder_db       | 2025-06-12 21:00:00.123 UTC [1] LOG:  database system is ready to accept connections
    reminder_backend  | INFO:     Started server process [1]
    reminder_backend  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
    reminder_frontend | INFO:     Started server process [1]
    reminder_frontend | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
    ```
2.  **Проверка:**
    Откройте браузер и перейдите по адресу `http://localhost`. Вы должны увидеть интерфейс приложения "Напоминалка". Попробуйте создать, отредактировать и удалить несколько записей.

3.  **Остановка:**
    В терминале, где запущен `docker-compose`, нажмите `Ctrl + C`. Это корректно остановит все контейнеры.

**Запуск в фоновом режиме:**
Чтобы запустить приложение и освободить терминал, используйте флаг `-d` (detached).
```bash
vselenaya@computer:~/reminder_app$ docker-compose up -d
[+] Running 3/3
 ✔ Container reminder_db       Started
 ✔ Container reminder_backend  Started
 ✔ Container reminder_frontend Started
```

**Просмотр логов в фоновом режиме:**
```bash
# Посмотреть логи всех сервисов и следить за ними в реальном времени
vselenaya@computer:~/reminder_app$ docker-compose logs -f

# Посмотреть логи только одного сервиса, например, бэкенда
vselenaya@computer:~/reminder_app$ docker-compose logs backend
```

**Полная остановка и очистка:**
Эта команда остановит и удалит контейнеры и сеть.
```bash
vselenaya@computer:~/reminder_app$ docker-compose down
[+] Running 3/3
 ✔ Container reminder_frontend Removed
 ✔ Container reminder_backend  Removed
 ✔ Container reminder_db       Removed
[+] Running 1/1
 ✔ Network reminder-app_reminder-net  Removed
```


#### 3.3. Возможные проблемы и их решения (Этап локальной контейнеризации)

В процессе "упаковки" приложения в контейнеры и их совместного запуска возник ряд характерных проблем. Этот раздел подробно описывает их и принятые решения.

<details>
  <summary><strong>Вопрос: `docker-compose up` падает с ошибкой `ImportError: attempted relative import with no known parent package` в логах контейнера `backend`.</strong></summary>

  > **Ответ:** Эта ошибка очень похожа на ту, что возникала при запуске тестов `pytest`, но ее причина кроется в структуре файлов внутри Docker-контейнера. `uvicorn` запускал файл `main.py` как отдельный скрипт, а не как часть Python-пакета, поэтому относительные импорты вида `from . import crud` не работали.
  >
  > **Решение:** Структура копирования в `Dockerfile` была изменена, чтобы сохранить структуру пакета внутри контейнера, и была скорректирована команда запуска:
  > -   **Было в `Dockerfile.backend`:** `COPY ./backend /app/` и `CMD ["uvicorn", "main:app", ...]`
  > -   **Стало:** `COPY ./backend /app/backend` и `CMD ["uvicorn", "backend.main:app", ...]`.
  > Теперь `uvicorn` запускает модуль `main` из пакета `backend` (который находится в `/app`), что позволяет Python корректно разрешать относительные импорты. Аналогичное изменение было внесено и в `Dockerfile.frontend` для консистентности.

</details>

<details>
  <summary><strong>Вопрос: `frontend` контейнер падает с ошибкой `RuntimeError: Form data requires "python-multipart" to be installed`.</strong></summary>

  > **Ответ:** Эта ошибка явно указывает на то, что для обработки данных из HTML-форм (`multipart/form-data`) фреймворку FastAPI требуется дополнительная библиотека `python-multipart`. Мы просто забыли добавить ее в зависимости для `frontend`-сервиса.
  >
  > **Решение:** В файл `requirements_frontend.txt` была добавлена строка:
  > ```
  > python-multipart
  > ```
  > После этого образы были пересобраны (`docker-compose up --build`), и ошибка исчезла.

</details>

<details>
  <summary><strong>Вопрос: `docker-compose up` падает со странной ошибкой `KeyError: 'ContainerConfig'`.</strong></summary>
  
  > **Ответ:** Эта редкая ошибка не связана с кодом приложения, а указывает на внутреннюю проблему Docker или `docker-compose`. Обычно она означает, что один из локально собранных образов или кэш сборки были повреждены.
  >
  > **Решение:** Проблема была решена путем полной очистки локального состояния Docker и принудительной пересборки образов без использования кэша. Это гарантирует, что все строится "с нуля".
  > 1.  Остановить все контейнеры: `docker-compose down`.
  > 2.  Удалить старые, собранные вручную образы: `docker rmi reminder_app_backend reminder_app_frontend`.
  > 3.  Выполнить полную системную очистку Docker: `docker system prune -a -f`. Эта команда удаляет все неиспользуемые контейнеры, сети, образы и кэш сборки.
  > 4.  Запустить сборку и запуск заново с флагом `--no-cache`:
  >     ```bash
  >     docker-compose up --build --no-cache
  >     ```
  > Эта процедура решила проблему.

</details>

<details>
  <summary><strong>Вопрос: `backend` контейнер циклически перезапускается (`CrashLoopBackOff`), а в логах ошибка `sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres-service" ... failed: Connection refused`.</strong></summary>
  
  > **Ответ:** Эта ошибка возникала после разделения на микросервисы. При старте `backend` немедленно пытался подключиться к базе данных. Однако контейнер с PostgreSQL, хоть и был запущен, требовал некоторого времени (5-10 секунд) на инициализацию, создание внутренних файлов и открытие порта для прослушивания. `backend` пытался подключиться к еще не готовой базе, получал отказ в соединении, падал, перезапускался и снова падал.
  >
  > **Решение:** В реальных системах для этого используют механизмы "ожидания" (например, скрипт `wait-for-it.sh`) или проверки готовности (`health checks`). В нашем `docker-compose.yml` уже есть базовый механизм — `depends_on`, который гарантирует, что `backend` начнет запускаться только после старта `db`. Однако `depends_on` не ждет, пока приложение *внутри* контейнера будет готово. В нашем случае `docker-compose` корректно перезапускал `backend` до тех пор, пока `postgres` не становился полностью готовым, после чего соединение устанавливалось успешно. Для устранения циклов падений можно было бы добавить в `backend` логику повторных попыток подключения при старте.

</details>

<details>
  <summary><strong>Вопрос: Почему `frontend`-сервис падал с ошибкой `jinja2.exceptions.UndefinedError: 'str object' has no attribute 'strftime'`?</strong></summary>

  > **Ответ:** Эта ошибка возникала после разделения на микросервисы. `Backend` возвращал данные в формате JSON, где все `datetime` объекты были преобразованы в обычные строки (например, `"2025-06-12T21:15:00"`). `Frontend` получал эти данные, передавал их в HTML-шаблон, где `Jinja2` пытался вызвать у этой **строки** метод `.strftime()`, которого у нее нет.
  >
  > **Решение:** В `frontend/main.py` была добавлена функция-помощник `parse_reminders`. Перед передачей данных в шаблон, эта функция "пробегается" по списку словарей от бэкенда и вручную конвертирует строковые представления дат обратно в полноценные объекты `datetime` с помощью `datetime.fromisoformat()`. Таким образом, в шаблон попадают уже правильные объекты, с которыми `strftime` может работать.

</details>



### Глава 4: Слой 2 — Базовое облачное развертывание (IaC)

На этом этапе мы совершаем ключевой переход от локальной разработки к приложению, работающему в интернете. Наша цель — **развернуть** все три микросервиса на **одной виртуальной машине (ВМ)** в облаке VK Cloud и сделать приложение доступным из любой точки мира по публичному, "белому" IP-адресу.

Ключевая технология этого этапа — **Инфраструктура как код (IaC)** с помощью инструмента **Terraform**. Вместо того чтобы вручную "накликивать" ВМ и настраивать сети в веб-интерфейсе облачного провайдера, мы опишем всю желаемую инфраструктуру в виде кода.

#### 4.1. Концепции этапа

##### Зачем нужен облачный провайдер? От `localhost` к публичному IP

Когда мы запускаем приложение локально, оно доступно только на нашем компьютере по адресу `http://localhost`. Никто другой не может к нему подключиться. Чтобы сделать приложение **глобально доступным**, нам нужен сервер, который:
1.  Работает 24/7.
2.  Имеет постоянный, публичный IP-адрес, по которому к нему можно обратиться из любой точки мира.

**VK Cloud** — это облачный провайдер, который предоставляет такие серверы (виртуальные машины) в аренду по запросу.

##### Что такое "Инфраструктура как код" (IaC) и Terraform?

**IaC (Infrastructure as Code)** — это подход к управлению IT-инфраструктурой, при котором конфигурация серверов, сетей, баз данных и других компонентов описывается с помощью кода, а не настраивается вручную.

**Terraform** — это один из самых популярных инструментов для реализации IaC. Он позволяет нам в текстовых файлах (с расширением `.tf`) декларативно описать, **что** мы хотим получить. Например: "Я хочу одну виртуальную машину с Ubuntu, 1 CPU и 1 GB RAM; одну приватную сеть; одну группу безопасности, разрешающую HTTP-трафик; один публичный IP-адрес, привязанный к этой машине". Terraform сам "договаривается" с API облачного провайдера (в нашем случае, VK Cloud) и выполняет все необходимые действия для создания или обновления инфраструктуры до желаемого состояния.

**Преимущества такого подхода:**
*   **Автоматизация**: Вся сложная инфраструктура создается одной командой.
*   **Воспроизводимость**: Можно в любой момент создать точную копию инфраструктуры, например, для тестирования.
*   **Версионирование**: Файлы конфигурации хранятся в Git, как и обычный код. Мы можем отслеживать все изменения, откатываться к предыдущим версиям и понимать, как инфраструктура эволюционировала.

##### Что такое Docker Hub и зачем он нам?

Чтобы наше приложение запустилось на новой, чистой виртуальной машине в облаке, этой машине нужно откуда-то скачать **Docker-образы** наших `frontend` и `backend` сервисов. Для этого используется **реестр контейнеров (Container Registry)**.

**Docker Hub** — это самый популярный, публичный реестр контейнеров (как GitHub для кода). Мы "собираем" наши образы на локальном компьютере, а затем "загружаем" (`docker push`) их в Docker Hub. После этого любая машина в мире, на которой есть Docker, сможет "скачать" (`docker pull`) и запустить наши образы по их уникальному имени.

#### 4.2. Пошаговое руководство по развертыванию

##### Шаг 1: Публикация Docker-образов в Docker Hub

1.  **Создайте аккаунт** на [hub.docker.com](https://hub.docker.com/). При регистрации вы придумаете `Docker ID` (имя пользователя), который станет частью имени ваших образов.

2.  **Авторизуйтесь в Docker Hub** через терминал на вашем локальном компьютере.
    ```bash
    vselenaya@computer:~/reminder_app$ docker login
    Authenticating with existing credentials...
    Login Succeeded
    ```

3.  **Соберите и загрузите (push) образы**. Команды выполняются из корневой папки проекта. Каждому образу присваивается тег в формате `<Docker_ID>/<имя_репозитория>:<версия>`.

    ```bash
    # Установите переменную для удобства. Замените vselenaya на свой Docker ID!
    vselenaya@computer:~/reminder_app$ export DOCKER_ID="vselenaya"

    # Собираем и загружаем образ бэкенда
    vselenaya@computer:~/reminder_app$ docker build -t ${DOCKER_ID}/reminder-backend:1.0 -f Dockerfile.backend .
    # ... вывод процесса сборки ...
    vselenaya@computer:~/reminder_app$ docker push ${DOCKER_ID}/reminder-backend:1.0
    The push refers to repository [docker.io/vselenaya/reminder-backend]
    ...
    1.0: digest: sha256:f31a1b... size: 1994

    # Аналогично собираем и загружаем образ фронтенда
    vselenaya@computer:~/reminder_app$ docker build -t ${DOCKER_ID}/reminder-frontend:1.0 -f Dockerfile.frontend .
    vselenaya@computer:~/reminder_app$ docker push ${DOCKER_ID}/reminder-frontend:1.0
    ```
4.  После успешной загрузки вы можете выйти из аккаунта командой `docker logout`.

##### Шаг 2: Настройка Terraform для работы с VK Cloud

1.  **Установите Terraform**. Скачайте бинарник с [официального сайта](https://www.terraform.io/downloads.html). Вы можете запускать его по полному пути (например, `~/Downloads/TERRAFORM/terraform`), не добавляя в системный `PATH`.

2.  **Настройте аккаунт VK Cloud**. Следуйте официальной инструкции VK Cloud, чтобы:
    -   Включить двухфакторную аутентификацию (обязательно для доступа по API).
    -   Скачать файл `terraform.rc`, переименовать его в `.terraformrc` и поместить в вашу домашнюю директорию (`~/`).
    -   Скачать файл `vkcs_provider.tf` и поместить его в папку `terraform/`.

3.  **Создайте SSH-ключ** (`ssh-keygen -t rsa`) и загрузите его **публичную** часть (`~/.ssh/id_rsa.pub`) в панель управления VK Cloud (в разделе "Облачные вычисления" -> "Ключи SSH").

4.  Создайте в папке `terraform/` файл `terraform.tfvars`. Этот файл предназначен для ваших личных данных и **никогда не должен попадать в систему контроля версий (Git)**.
    ```tfvars
    vk_username   = "sbest-user@bmail.ru"
    vk_project_id = "1135507f7b99490cbbee150c2ebee1ae"
    key_pair_name = "my-ssh-key-vkcloud"
    ```
    *Примечание:* Мы намеренно не указываем здесь пароль (`vk_password`). Terraform запросит его в консоли при запуске, что более безопасно.

##### Шаг 3: Создание облачной инфраструктуры

1.  **Инициализация Terraform**. Перейдите в директорию `terraform/` и выполните команду один раз для проекта.
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform init
    Initializing the backend...
    Initializing provider plugins...
    - Finding vk-cs/vkcs versions matching "< 1.0.0"...
    - Installing vk-cs/vkcs v0.10.0...
    Terraform has been successfully initialized!
    ```

2.  **Применение конфигурации**. Эта команда создаст все ресурсы, описанные в `.tf` файлах.
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform apply
    var.vk_password
      Пароль от аккаунта VK Cloud

      Enter a value: [введите ваш пароль от VK Cloud и нажмите Enter]

    # ... Terraform покажет подробный план того, что он собирается создать ...
    Plan: 8 to add, 0 to change, 0 to destroy.
    Do you want to perform these actions?
      Enter a value: yes

    vkcs_networking_network.app_net: Creating...
    vkcs_networking_secgroup.app_secgroup: Creating...
    ...
    vkcs_compute_instance.app_vm: Creation complete after 1m52s [id=ac7d60b7-655f-4f07-ba0c-7465e79e6d5f]
    ...
    Apply complete! Resources: 8 added, 0 changed, 0 to destroy.

    Outputs:

    instance_public_ip = "83.166.238.144"
    ```
    Сохраните публичный IP-адрес (`83.166.238.144` в примере). Внутри файла `main.tf` есть блок `user_data`, который при первом запуске ВМ автоматически установит на нее Docker и Docker Compose.

##### Шаг 4: Запуск приложения на созданной ВМ

1.  **Подключитесь к ВМ по SSH**:
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ssh ubuntu@83.166.238.144
    Welcome to Ubuntu 22.04.3 LTS ...
    ```
2.  **Создайте `docker-compose.yml` на ВМ** с помощью текстового редактора `nano`. Вставьте в него содержимое вашего локального файла `docker-compose.yml`, но **с ключевым изменением**: замените секции `build:` на `image:` с полными именами образов из вашего Docker Hub.

3.  **Запустите приложение** в фоновом режиме (`-d`).
    ```bash
    ubuntu@reminder-app-vm:~$ sudo docker-compose up -d
    [+] Running 3/3
     ✔ Container reminder_db       Started
     ✔ Container reminder_backend  Started
     ✔ Container reminder_frontend Started
    ```
4.  **Проверьте статус** контейнеров: `sudo docker-compose ps`.
    ```bash
    ubuntu@reminder-app-vm:~$ sudo docker-compose ps
    NAME                COMMAND                  SERVICE             STATUS              PORTS
    reminder_backend    "uvicorn backend.mai…"   backend             running             8000/tcp
    reminder_db         "docker-entrypoint.s…"   db                  running             5432/tcp
    reminder_frontend   "uvicorn frontend.ma…"   frontend            running             0.0.0.0:80->8000/tcp
    ```
    Теперь ваше приложение работает в облаке и доступно любому в интернете по адресу `http://83.166.238.144`.

##### Шаг 5: Полный откат и очистка
Чтобы остановить потребление облачных ресурсов, необходимо полностью удалить созданную инфраструктуру.

1.  На **вашем локальном компьютере** в папке `terraform/` выполните:
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform destroy

    # ... Terraform запросит пароль и подтверждение ('yes') ...

    Destroy complete! Resources: 8 destroyed.
    ```
    Эта команда аккуратно удалит все созданные в VK Cloud ресурсы.


#### 4.3. Возможные проблемы и их решения (Этап базового облачного развертывания)

Развертывание инфраструктуры в облаке с помощью Terraform — мощный, но требующий точности процесс. На этом пути мы столкнулись с рядом характерных проблем, отладка которых позволила выработать финальную рабочую конфигурацию.

<details>
  <summary><strong>Вопрос: `terraform apply` падает с ошибкой `Invalid resource type` для `vkcs_compute_secgroup` или `vkcs_compute_secgroup_v2`.</strong></summary>

  > **Ответ:** Это была самая сложная и долгая часть отладки. Проблема заключалась в том, что имена ресурсов Terraform для VK Cloud оказались неочевидными и отличались от стандартных имен OpenStack.
  >
  > **Процесс решения:**
  > 1.  Изначально были предприняты попытки использовать имена с суффиксом `_v2` (например, `vkcs_compute_secgroup_v2`), так как это является стандартом для многих ресурсов API v2. Это не сработало.
  > 2.  Затем были предприняты попытки использовать имена без суффикса (`vkcs_compute_secgroup`), что также приводило к ошибке.
  > 3.  **Ключевым моментом** стало изучение официальной документации VK Cloud по созданию групп безопасности ([`PDF про secgroup`](https://cloud.vk.com/docs/tools-for-using-services/terraform/how-to-guides/vnet/secgroups#2224-tabpanel-3)). В ней были указаны точные и правильные имена ресурсов:
  >     -   Для создания группы безопасности: `vkcs_networking_secgroup`
  >     -   Для создания правила внутри группы: `vkcs_networking_secgroup_rule`
  >
  > **Финальное решение:** В `main.tf` был добавлен ресурс `vkcs_networking_secgroup` для создания нашей основной группы `app-reminders-secgroup-tf`, а также два ресурса `vkcs_networking_secgroup_rule` для добавления в нее правил, разрешающих трафик на порты `22` (SSH) и `80` (HTTP).

</details>

<details>
  <summary><strong>Вопрос: Terraform выдает ошибку `Your query returned no results` при поиске сети или образа.</strong></summary>

  > **Ответ:** Эта ошибка означает, что Terraform, используя `data` блок, не смог найти в вашем облачном проекте ресурс с указанным именем или свойствами. В нашем случае было две таких проблемы:
  >   1.  **Сеть:** Изначально в `network.tf` для поиска внешней сети использовалось имя `"ext-net"`. Однако в учебном окружении VK Cloud она называется `"internet"`. После исправления имени в `data "vkcs_networking_network" "extnet"` Terraform смог ее найти.
  >   2.  **Образ Ubuntu:** Простой поиск по имени `name = "Ubuntu-22.04-LTS"` не давал результатов. **Решение** было найдено в официальной документации ([`PDF про создание ВМ`](https://cloud.vk.com/docs/tools-for-using-services/terraform/how-to-guides/iaas/create#4170-tabpanel-0)). Необходимо было использовать более сложный поиск по свойствам образа:
  >       ```terraform
  >       data "vkcs_images_image" "ubuntu" {
  >         visibility  = "public"
  >         most_recent = true
  >         properties = {
  >           mcs_os_distro  = "ubuntu"
  >           mcs_os_version = "22.04"
  >         }
  >       }
  >       ```
  >       Этот метод поиска оказался более надежным и сработал корректно.

</details>

<details>
  <summary><strong>Вопрос: Terraform выдает ошибку `Unable to find security_group with name or id 'ssh'`.</strong></summary>

  > **Ответ:** Эта ошибка возникла, когда мы пытались применить к виртуальной машине преднастроенную группу безопасности `ssh`, предполагая, что она существует в проекте по умолчанию. Ошибка показала, что в данном конкретном проекте VK Cloud такой группы нет.
  >
  > **Решение:** Вместо того чтобы полагаться на существующие группы, мы явно создаем собственную группу (`vkcs_networking_secgroup "app_secgroup"`) и добавляем в нее все необходимые правила (для порта 22 и 80). Затем мы применяем к ВМ именно эту, созданную нами группу. Это делает конфигурацию самодостаточной и не зависящей от начального состояния проекта.

</details>

<details>
  <summary><strong>Вопрос: При `terraform destroy` возникает ошибка `Duplicate data "vkcs_networking_network" configuration`.</strong></summary>
  
  > **Ответ:** Это была простая, но частая ошибка, возникающая при рефакторинге. Она означала, что блок `data "vkcs_networking_network" "extnet"` был объявлен дважды в разных `.tf` файлах (например, и в `main.tf`, и в `network.tf`). Terraform читает все `.tf` файлы в директории и объединяет их, поэтому не допускает дублирования имен ресурсов или data-блоков.
  >
  > **Решение:** После обнаружения и удаления дублирующегося блока из одного из файлов команда `destroy` отработала корректно.

</details>

<details>
  <summary><strong>Вопрос: При `terraform destroy` или `apply` он запрашивает пароль, но затем выдает `Authentication failed`.</strong></summary>

  > **Ответ:** Эта ошибка почти всегда означает, что был введен неверный пароль от аккаунта VK Cloud. Важно вводить его внимательно, так как он не отображается в консоли. Также стоит убедиться, что вы не пытаетесь выполнить команду из другой директории, так как `terraform` может не найти нужные файлы конфигурации.

</details>

<details>
  <summary><strong>Вопрос: Почему был выбран публичный Docker Hub, а не приватный реестр VK Cloud, который интегрирован с облаком?</strong></summary>

  > **Ответ:** Использование приватного реестра VK Cloud потребовало бы дополнительной настройки аутентификации на виртуальной машине, чтобы `docker` или `docker-compose` внутри ВМ имели права на скачивание образов. Это усложнило бы `user_data` скрипт. Для упрощения и универсальности проекта был выбран публичный Docker Hub, доступ к образам в котором не требует аутентификации при скачивании (`docker pull`). Это позволило сделать скрипт запуска на ВМ максимально простым.

</details>



### Глава 5: Слой 3 — Оркестрация с Kubernetes

На этом финальном этапе мы переходим от развертывания на одной виртуальной машине к полноценной **оркестрации контейнеров** с помощью **Kubernetes**. Это стандарт де-факто в индустрии для управления сложными, распределенными приложениями. Наша цель — развернуть приложение в отказоустойчивом, масштабируемом и самовосстанавливающемся окружении, которое предоставляет управляемый кластер Kubernetes в VK Cloud.

#### 5.1. Концепции этапа

##### Зачем нужен Kubernetes, если на ВМ с Docker Compose всё работало?

Развертывание на одной ВМ имеет критические недостатки, которые становятся очевидными при росте приложения или требований к его надежности:
-   **Нет отказоустойчивости**: Если виртуальная машина "упадет" (из-за сбоя оборудования, ошибки ОС и т.д.), все приложение полностью перестанет работать до тех пор, пока инженер не починит ее вручную.
-   **Сложность масштабирования**: Если нагрузка на приложение возрастет, единственным выходом будет "вертикальное масштабирование" (покупка более мощной ВМ, что дорого и требует перезагрузки) или очень сложное "горизонтальное масштабирование" вручную (создание еще одной ВМ, настройка балансировщика нагрузки и т.д.).
-   **Сложность обновлений**: Чтобы обновить приложение, нужно останавливать старые контейнеры и запускать новые, что приводит к простою (downtime).

**Kubernetes** решает все эти проблемы. Это "оркестратор" — умная система, которая управляет целой группой (кластером) виртуальных машин, как единым целым.

-   **Самовосстановление**: Если одна из ВМ (в Kubernetes она называется **Нода**) или один из контейнеров (**Под**) выйдет из строя, Kubernetes это заметит и автоматически перезапустит его на другой, здоровой ноде.
-   **Масштабируемость**: Kubernetes позволяет легко масштабировать приложение, просто изменив одну цифру в конфигурации (`replicas: 3`), и он сам запустит нужное количество копий.
-   **Балансировка нагрузки**: Он имеет встроенные механизмы для распределения трафика между всеми работающими копиями вашего приложения.
-   **Плавные обновления (Rolling Updates)**: При обновлении приложения Kubernetes сначала запускает новую версию контейнера, дожидается, пока она станет готова, переключает на нее трафик и только потом останавливает старую. Все это происходит без простоя для пользователя.

##### Ключевые объекты Kubernetes, которые мы используем

Вместо одного файла `docker-compose.yml`, в Kubernetes используется набор YAML-файлов, называемых **Манифестами**. Каждый манифест описывает один объект.

-   **`Pod`**: Самая маленькая единица развертывания. Это "обертка" для одного или нескольких контейнеров.
-   **`Deployment`**: Более высокоуровневый объект, который управляет Подами. Он следит за тем, чтобы всегда было запущено нужное количество копий (`replicas`) вашего приложения, и управляет процессом их обновления.
-   **`Service`**: Предоставляет Подам стабильный сетевой адрес.
    -   `type: ClusterIP`: Создает адрес, доступный **только внутри** кластера. Мы используем его для `backend` и `postgres`.
    -   `type: LoadBalancer`: Создает облачный балансировщик нагрузки и выдает **публичный IP-адрес** для доступа извне. Мы используем его для `frontend`.
-   **`PersistentVolumeClaim` (PVC)**: Запрос на выделение постоянного дискового пространства. Необходим для stateful-приложений, таких как базы данных.

#### 5.2. Terraform: создание Kubernetes-кластера в VK Cloud

Мы снова используем Terraform, но с конфигурацией, которая создает не одну ВМ, а целый **управляемый кластер Kubernetes**. Это означает, что VK Cloud берет на себя управление "мозгами" кластера (master-нодами), а мы управляем только "рабочими" нодами, на которых запускаются наши приложения.

**Процесс создания кластера:**
1.  Убедитесь, что в папке `terraform/` находится актуальная версия `main.tf` для создания Kubernetes. Эта конфигурация описывает:
    -   `resource "vkcs_kubernetes_cluster"`: Создает сам кластер.
    -   `resource "vkcs_kubernetes_node_group"`: Создает группу из одной или нескольких рабочих нод (виртуальных машин), которые присоединяются к кластеру.
2.  Выполните `terraform destroy` (чтобы удалить ВМ от предыдущего этапа, если она осталась) и затем `terraform apply`.
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform apply
    var.vk_password
      Пароль от аккаунта VK Cloud

      Enter a value: [введите ваш пароль от VK Cloud и нажмите Enter]
    
    # ... Terraform покажет план создания 6 ресурсов (кластер, группа нод, сеть и т.д.) ...
    Plan: 6 to add, 0 to change, 0 to destroy.
    Do you want to perform these actions?
      Enter a value: yes
    
    # ... Процесс создания займет 10-20 минут ...
    vkcs_kubernetes_cluster.k8s_cluster: Creation complete after 9m7s [id=e0929247-c7dc-4ee9-ab3d-019fc5a65057]
    vkcs_kubernetes_node_group.k8s_node_group: Creation complete after 1m52s [id=54aa4235-ad11-4821-b719-e385d6c61614]
    
    Apply complete! Resources: 6 added, 0 changed, 0 destroyed.

    Outputs:

    cluster_id = "e0929247-c7dc-4ee9-ab3d-019fc5a65057"
    get_kubeconfig_command = "vkcs kubernetes cluster get-kubeconfig e0929247-... --file ./kubeconfig.yaml"
    ```

#### 5.3. Настройка `kubectl` и доступ к кластеру

`kubectl` — это основной инструмент для управления кластером из командной строки.

1.  **Установите `kubectl`** и **плагин `client-keystone-auth`** от VK Cloud, следуя их официальной документации.
2.  **Получите `kubeconfig`**: `kubeconfig` — это файл- "паспорт" для доступа к вашему кластеру. Выполните команду, которую выдал Terraform, или скачайте файл из веб-интерфейса VK Cloud. В папке `terraform/` появится файл `kubeconfig.yaml`.
3.  **Ограничьте права на `kubeconfig`**:
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ chmod 600 ./kubeconfig.yaml
    ```
4.  **Укажите `kubectl`, где найти конфигурацию**: Это ключевой шаг, который нужно выполнять в каждой новой сессии терминала.
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ export KUBECONFIG=$(pwd)/kubeconfig.yaml
    ```
5.  **Проверьте подключение**:
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ kubectl get nodes
    Please enter password: [введите ваш пароль от VK Cloud]
    NAME                                  STATUS   ROLES    AGE   VERSION
    reminder-app-cluster-master-0         Ready    master   37m   v1.28.9
    reminder-app-cluster-worker-nodes-0   Ready    <none>   31m   v1.28.9
    ```
    Если вы видите свои ноды, значит, вы успешно подключились к кластеру.

#### 5.4. Развертывание приложения в Kubernetes

Мы используем YAML-манифесты из папки `k8s/` для декларативного описания нашего приложения. Убедитесь, что в k8s/*.yaml указаны образы из вашего Docker Hub (DOCKER_ID/tags), а не локальные.

1.  **Примените манифесты**: Из корневой папки проекта выполните:
    ```bash
    vselenaya@computer:~/reminder_app$ kubectl apply -f k8s/
    deployment.apps/backend-deployment created
    service/backend-service created
    deployment.apps/postgres-deployment created
    service/postgres-service created
    deployment.apps/frontend-deployment created
    service/frontend-service created
    ```
    *(На этом этапе мы еще не применяем HPA и мониторинг).*

2.  **Проверьте результат**:
    -   Наблюдайте за созданием Подов: `kubectl get pods -w`. Дождитесь, пока все перейдут в статус `Running`.
    -   Узнайте публичный IP-адрес: `kubectl get services frontend-service`. Дождитесь, пока у `frontend-service` в колонке `EXTERNAL-IP` появится IP-адрес.
        ```
        vselenaya@computer:~/reminder_app$ kubectl get services frontend-service
        NAME               TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
        frontend-service   LoadBalancer   10.254.48.124    146.185.210.210   80:30134/TCP   2m47s
        ```
    -   Откройте этот IP-адрес в браузере. Приложение должно работать.

#### 5.5. Уничтожение ресурсов

Чтобы после завершения работы удалить приложение из кластера, а затем и сам кластер:

1.  **Удаление приложения из Kubernetes**:
    ```bash
    vselenaya@computer:~/reminder_app$ kubectl delete -f k8s/
    deployment.apps "backend-deployment" deleted
    ...
    ```
2.  **Уничтожение инфраструктуры в VK Cloud**:
    ```bash
    vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform destroy
    ...
    Destroy complete! Resources: 6 destroyed.
    ```



#### 5.6. Возможные проблемы и их решения (Этап развертывания в Kubernetes)

Переход от `docker-compose` к Kubernetes — это качественный скачок в сложности, который сопровождался рядом специфических проблем. Этот раздел подробно описывает процесс их диагностики и решения.

<details>
  <summary><strong>Вопрос: `terraform apply` падает с ошибкой `Unsupported attribute ... "kube_configs"`.</strong></summary>
  
  > **Ответ:** Эта ошибка возникла при попытке получить конфигурацию для доступа к кластеру (`kubeconfig`) напрямую из ресурса Terraform. Оказалось, что провайдер `vkcs` не экспортирует этот атрибут, в отличие от провайдеров других облаков.
  >
  > **Решение:** Согласно официальной документации VK Cloud ([`PDF про создание кластера`](https://cloud.vk.com/docs/kubernetes/k8s/service-management/create-cluster/create-terraform#5957-tabpanel-0)), правильный способ — это получить ID созданного кластера и затем использовать утилиту командной строки `vkcs` для скачивания `kubeconfig`. Блок `output` в `main.tf` был исправлен, чтобы вместо неработающей конструкции он выводил готовую команду для выполнения в терминале:
  > ```terraform
  > output "get_kubeconfig_command" {
  >   description = "Команда для скачивания файла конфигурации kubectl..."
  >   value       = "vkcs kubernetes cluster get-kubeconfig ${vkcs_kubernetes_cluster.k8s_cluster.id} --file ./kubeconfig.yaml"
  > }
  > ```

</details>

<details>
  <summary><strong>Вопрос: `terraform apply` падает с ошибкой `Flavor ... not available. Need master flavor with more resources, required RAM: 6144, vCPU: 2`.</strong></summary>
  
  > **Ответ:** Сообщение об ошибке было предельно ясным: выбранный тип виртуальной машины (`flavor`) для мастер-ноды Kubernetes не соответствовал минимальным системным требованиям для выбранной версии Kubernetes (v1.28).
  >
  > **Решение:** В файле `terraform/variables.tf` значение по умолчанию для переменной `k8s_master_flavor_name` было изменено с `"STD2-2-4"` (2 CPU, 4GB RAM) на `"STD2-2-8"` (2 CPU, 8GB RAM), что удовлетворило требованиям облака. После очистки (`terraform destroy`) и повторного запуска `terraform apply` кластер был успешно создан.

</details>

<details>
  <summary><strong>Вопрос: При попытке выполнить `kubectl get nodes` возникает ошибка `The connection to the server localhost:8080 was refused`.</strong></summary>
  
  > **Ответ:** Эта классическая ошибка означает, что `kubectl` не знает, к какому кластеру ему нужно подключиться, и пытается использовать адрес по умолчанию (`localhost:8080`), который обычно используется для локальных прокси.
  >
  > **Причина:** Переменная окружения `KUBECONFIG`, которая указывает путь к файлу с учетными данными кластера, не была установлена в текущей сессии терминала (например, после перезагрузки компьютера или открытия нового окна терминала).
  >
  > **Решение:** **Перед каждым сеансом работы с `kubectl`** необходимо выполнять команду, устанавливающую эту переменную:
  > ```bash
  > # Выполнять из папки, где лежит kubeconfig.yaml (в нашем случае - terraform/)
  > export KUBECONFIG=$(pwd)/kubeconfig.yaml
  > ```
  > После этого `kubectl` находит правильные учетные данные и успешно подключается к вашему кластеру в VK Cloud.

</details>

<details>
  <summary><strong>Вопрос: После `kubectl apply -f k8s/` приложение не работает, а `kubectl get pods` показывает, что `postgres` в статусе `Pending`, а `backend` — в `CrashLoopBackOff`.</strong></summary>
  
  > **Ответ:** Это была двухэтапная проблема, где одна ошибка вызывала другую.
  > 1.  **`postgres` в `Pending`**: Статус `Pending` для пода, запрашивающего диск (`PersistentVolumeClaim`), почти всегда означает, что Kubernetes не может выделить ему этот диск. В управляемых кластерах это часто происходит из-за отсутствия или неправильной конфигурации `StorageClass`, который отвечает за динамическое создание дисков.
  > 2.  **`backend` в `CrashLoopBackOff`**: Этот статус означает, что контейнер запускается, тут же падает с ошибкой, и Kubernetes пытается его перезапустить снова и снова. Просмотр логов (`kubectl logs backend-... --previous`) показал ошибку `connection to server at "postgres-service" ... failed: Connection refused`. `backend` падал, потому что не мог подключиться к базе данных, которая еще даже не запустилась.
  >
  > **Решение:** Для учебного проекта, где сохранность данных не является критичной, было принято решение упростить конфигурацию хранилища для PostgreSQL. В `k8s/database.yaml` использование `PersistentVolumeClaim` было заменено на **`emptyDir: {}`**. `emptyDir` — это временная директория, которая создается вместе с Подом и живет, пока жив Под (emptyDir переживает перезапуск контейнера в том же поде, но теряется при пересоздании/переназначении пода на другую ноду). Это не требует настройки дисков в облаке. Но это временное решение, для сохранности данных заменить на PVC со StorageClass’ом, когда будет возможность.
  >
  > После применения этого изменения (`kubectl apply -f k8s/database.yaml`) под `postgres` успешно запустился. Вслед за ним, при очередном автоматическом перезапуске, успешно стартовал и `backend`, так как теперь он смог найти свою базу данных.

</details>

<details>
  <summary><strong>Вопрос: Почему был нужен плагин `client-keystone-auth`? kubectl не работает напрямую?</strong></summary>

  > **Ответ:** Это особенность аутентификации в облаках на базе OpenStack, к которым относится и VK Cloud. Стандартный `kubeconfig` часто использует токены, которые имеют короткий срок жизни. Чтобы не получать новый токен вручную каждые несколько часов, используется специальный плагин-помощник.
  >
  > В нашем `kubeconfig.yaml` в секции `user` есть блок `exec`. Он говорит `kubectl`: "Перед тем как сделать запрос к API кластера, сначала запусти команду `client-keystone-auth` с вот этими параметрами (твой логин, project_id и т.д.). Эта команда сходит в API VK Cloud, получит свежий, короткоживущий токен и вернет его мне. А уже с этим токеном я пойду в API Kubernetes". Таким образом, процесс получения токенов автоматизируется и становится прозрачным для пользователя.

</details>












### Глава 6: Слой 4 — Продвинутая настройка: Мониторинг и доступ через Ingress

На этом заключительном этапе мы добавляем в наш кластер профессиональную систему мониторинга и настраиваем более элегантный способ доступа к ее веб-интерфейсу. Мы не будем ограничиваться простым развертыванием, а реализуем сложную, многокомпонентную схему, близкую к реальным продакшн-системам.

#### 6.1. Концепции этапа

##### Мониторинг с Prometheus, Grafana и Trickster

-   **Prometheus**: "Сердце" нашей системы мониторинга. Это база данных временных рядов, которая регулярно "опрашивает" (делает scrape) различные источники (наши Поды, Ноды, API Kubernetes) и собирает с них числовые **метрики** (загрузка CPU, количество HTTP-запросов и т.д.).
-   **Grafana**: "Лицо" нашего мониторинга. Это мощный инструмент для визуализации данных. Grafana подключается к Prometheus как к источнику данных и позволяет строить наглядные **дашборды** с графиками, диаграммами и алертами.
-   **Trickster**: Это **кэширующий реверс-прокси**, который мы ставим между Grafana и Prometheus. Когда Grafana запрашивает у Prometheus данные для построения графика, Trickster перехватывает этот запрос. Если такой же запрос уже был недавно, Trickster отдает данные из своего кэша, не нагружая Prometheus. Это значительно ускоряет работу дашбордов и снижает нагрузку на систему мониторинга, особенно когда дашборды просматривает много людей.

##### Доступ через Ingress-Nginx

До сих пор для доступа к приложению извне мы использовали `Service` типа `LoadBalancer`, который заказывал у VK Cloud отдельный облачный балансировщик и публичный IP-адрес. Это просто, но неэффективно, если у нас много сервисов (Grafana, Prometheus, наше приложение) — на каждый понадобится свой дорогой балансировщик.

**Ingress Controller** решает эту проблему. Это "умный роутер" внутри нашего кластера.
1.  Мы создаем **один-единственный** `Service` типа `LoadBalancer` для самого Ingress-контроллера Nginx. Он будет нашей единой точкой входа в кластер.
2.  Затем мы создаем правила (`Ingress`-ресурсы), которые говорят этому роутеру, как распределять входящий трафик в зависимости от **доменного имени (хоста)**. Например: "Если пришел запрос на `grafana.example.com`, отправь его на сервис Grafana. Если на `app.example.com`, отправь на сервис нашего приложения".

Это позволяет "публиковать" множество сервисов через один IP-адрес и один балансировщик.

#### 6.2. Установка стека мониторинга с помощью Helm

Для установки мы используем **Helm**, но не просто выполняем `helm install`, а следуем более гибкому подходу: скачиваем чарт, кастомизируем его конфигурацию (`values.yaml`) и затем устанавливаем из локальных файлов. Все команды выполняются из директории `helms/` внутри проекта.

##### Шаг 1: Установка Prometheus

1.  **Добавьте репозиторий и скачайте чарт**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    "prometheus-community" has been added to your repositories
    vselenaya@computer:~/reminder_app/helms$ helm repo update
    Hang tight while we grab the latest from your chart repositories...
    ...Successfully got an update from the "prometheus-community" chart repository
    vselenaya@computer:~/reminder_app/helms$ helm pull prometheus-community/prometheus
    vselenaya@computer:~/reminder_app/helms$ tar zxf prometheus-27.12.0.tgz
    ```
2.  **Подготовьте файл конфигурации**:
    Мы копируем стандартный `values.yaml` из чарта, чтобы использовать его как основу для наших настроек.
    ```bash
    vselenaya@computer:~/reminder_app/helms$ cp prometheus/values.yaml prometheus-values.yaml
    ```
3.  **Отредактируйте `prometheus-values.yaml`**:
    Откройте `prometheus-values.yaml` в текстовом редакторе. В этом файле тысячи строк настроек. Мы изменим только несколько ключевых, чтобы упростить установку для учебного проекта:
    -   Найдите секцию `alertmanager` и установите `enabled: false`.
    -   Найдите секцию `pushgateway` и установите `enabled: false`.
    -   Найдите секцию `server.persistentVolume` и установите `enabled: false`.
    Это отключит ненужные нам компоненты (менеджер алертов, шлюз для push-метрик) и использование постоянного диска, упростив запуск.

4.  **Установите Prometheus**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm upgrade --install --create-namespace --values prometheus-values.yaml prometheus -n monitoring prometheus-community/prometheus
    Release "prometheus" does not exist. Installing it now.
    NAME: prometheus
    LAST DEPLOYED: Thu Jun 13 11:00:00 2025
    NAMESPACE: monitoring
    STATUS: deployed
    ...
    ```

##### Шаг 2: Установка Grafana

Процесс аналогичен.

1.  **Добавьте репозиторий и скачайте чарт**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm repo add grafana https://grafana.github.io/helm-charts
    vselenaya@computer:~/reminder_app/helms$ helm repo update
    vselenaya@computer:~/reminder_app/helms$ helm pull grafana/grafana
    vselenaya@computer:~/reminder_app/helms$ tar zxf grafana-8.14.2.tgz
    ```
2.  **Подготовьте и отредактируйте файл конфигурации `grafana-values.yaml`**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ cp grafana/values.yaml grafana-values.yaml
    ```
    Откройте `grafana-values.yaml` и найдите секцию `persistence`. Измените ее, чтобы включить использование постоянного диска (PVC). Это позволит сохранять созданные вами дашборды даже после перезапуска Пода Grafana.
    ```yaml
    persistence:
      type: pvc
      enabled: true
      # ... остальные параметры persistence можно оставить по умолчанию
    ```

3.  **Установите Grafana**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm upgrade --install --create-namespace --values grafana-values.yaml grafana -n monitoring grafana/grafana
    Release "grafana" does not exist. Installing it now.
    NAME: grafana
    ...
    STATUS: deployed
    ```

##### Шаг 3: Установка Trickster (Кэширующий прокси)

1.  **Добавьте репозиторий и скачайте чарт**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm repo add tricksterproxy https://helm.tricksterproxy.io
    vselenaya@computer:~/reminder_app/helms$ helm repo update
    vselenaya@computer:~/reminder_app/helms$ helm pull tricksterproxy/trickster
    vselenaya@computer:~/reminder_app/helms$ tar zxf trickster-1.5.4.tgz
    ```
2.  **Подготовьте `trickster-values.yaml` и установите Trickster**:
    В данном случае можно использовать стандартные настройки.
    ```bash
    vselenaya@computer:~/reminder_app/helms$ cp trickster/values.yaml trickster-values.yaml
    vselenaya@computer:~/reminder_app/helms$ helm upgrade --install --create-namespace --values trickster-values.yaml trickster -n monitoring tricksterproxy/trickster
    ```

##### Шаг 4: Установка Ingress-Nginx Controller

Это будет нашей единой точкой входа в кластер.

1.  **Добавьте репозиторий**:
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    vselenaya@computer:~/reminder_app/helms$ helm repo update
    ```
2.  **Установите Ingress Controller**:
    Мы устанавливаем его в отдельный неймспейс `ingress-nginx` и с помощью `--set` указываем, что его сервис должен быть типа `LoadBalancer`, чтобы получить публичный IP.
    ```bash
    vselenaya@computer:~/reminder_app/helms$ helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
      --namespace ingress-nginx \
      --create-namespace \
      --set controller.service.type=LoadBalancer \
      --set controller.service.externalTrafficPolicy=Local
    ```

##### Шаг 5: Настройка доступа к Grafana через Ingress

1.  **Узнайте публичный IP** вашего Ingress-контроллера.
    ```bash
    vselenaya@computer:~/reminder_app$ kubectl get service -n ingress-nginx ingress-nginx-controller
    NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE
    ingress-nginx-controller   LoadBalancer   10.254.12.34    89.208.86.40     80:31189/TCP,443:30198/TCP   5m
    ```
2.  **Создайте манифест `grafana-ingress.yaml`**:
    Этот файл описывает правило маршрутизации. Мы используем специальный сервис `sslip.io`, который преобразует любой IP-адрес в доменное имя.
    ```yaml
    # helms/grafana-ingress.yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress 
    metadata:
      name: grafana
      namespace: monitoring
    spec:
      rules:
      - host: monitoring.95-163-250-30.sslip.io # вот здесь записываете <EXTERNAL-IP> в формате через дефис
        http:
          paths:
          - path: / 
            pathType: Prefix
            backend:
              service:
                name: grafana # Имя сервиса Grafana
                port:
                  number: 80 # Порт сервиса Grafana
    ```
3.  **Примените Ingress**:
    ```bash
    vselenaya@computer:~/reminder_app$ kubectl apply -f helms/grafana-ingress.yaml
    ingress.networking.k8s.io/grafana created
    ```

4.  **Получите пароль Grafana и зайдите в веб-интерфейс**:
    ```bash
    vselenaya@computer:~/reminder_app$ kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
    super-secret-password-123
    ```
    Теперь откройте в браузере адрес `http://monitoring.95-163-250-30.sslip.io`. Используйте логин `admin` и полученный пароль. Вы должны увидеть приветственную страницу Grafana.

#### 6.3. Подключение приложения "Напоминалка" к мониторингу

Чтобы Prometheus начал собирать метрики не только с подов, но ещё и с нашего `backend`-сервиса, нужно "познакомить" их.
1.  **Добавьте экспортёр метрик в `backend`**:
    -   Добавьте библиотеку `prometheus-fastapi-instrumentator` в `requirements_backend.txt`.
    -   Добавьте код `Instrumentator().instrument(app).expose(app)` в `backend/main.py`.
2.  **Соберите и загрузите новую версию образа** (`.../reminder-backend:1.1`).
3.  **Обновите `Deployment` в Kubernetes**:
    -   Измените тег образа в `k8s/backend.yaml` на `1.1`.
    -   В `Service` для `backend` добавьте имя для порта `name: http`.
    -   Примените изменения: `kubectl apply -f k8s/backend.yaml`.
4.  **Создайте и примените `monitoring/backend-monitor.yaml`**:
    Этот файл создает ресурс `ServiceMonitor`, который указывает Prometheus опрашивать сервис с меткой `app: backend` по порту с именем `http` и пути `/metrics`.

После выполнения этих шагов вы сможете зайти в Grafana, найти готовые дашборды (например, "Kubernetes / Compute Resources / Pod") и увидеть графики потребления CPU/памяти вашими подами, а также построить свои графики на основе метрик вашего приложения (например, `http_requests_total`). Но я этого не делал, у меня метрики только с подов собираются.





#### 6.4. Возможные проблемы и их решения (Этап оркестрации и мониторинга)

Переход от `docker-compose` к Kubernetes — это качественный скачок в сложности, который сопровождался рядом специфических проблем. Этот раздел подробно описывает процесс их диагностики и решения.

<details>
  <summary><strong>Вопрос: `terraform apply` падает с ошибкой `Flavor ... not available. Need master flavor with more resources...`.</strong></summary>

  > **Ответ:** Сообщение об ошибке было предельно ясным: выбранный тип виртуальной машины (`flavor`) для мастер-ноды Kubernetes не соответствовал минимальным системным требованиям для выбранной версии Kubernetes (v1.28).
  >
  > **Решение:** В файле `terraform/variables.tf` значение по умолчанию для переменной `k8s_master_flavor_name` было изменено с `"STD2-2-4"` (2 CPU, 4GB RAM) на `"STD2-2-8"` (2 CPU, 8GB RAM), что удовлетворило требованиям облака. После очистки (`terraform destroy`) и повторного запуска `terraform apply` кластер был успешно создан.
  > ```bash
  > vselenaya@computer:~/reminder_app/terraform$ ~/Downloads/TERRAFORM/terraform apply
  > ...
  > Error: error creating vkcs_kubernetes_cluster: ... "Flavor ... not available. Need master flavor with more resources, required RAM: 6144, vCPU: 2"
  > ...
  > ```
  > Это хороший пример того, как важно внимательно читать сообщения об ошибках от облачных провайдеров.

</details>

<details>
  <summary><strong>Вопрос: При попытке выполнить `kubectl get nodes` возникает ошибка `The connection to the server localhost:8080 was refused`.</strong></summary>
  
  > **Ответ:** Эта классическая ошибка означает, что `kubectl` не знает, к какому кластеру ему нужно подключиться, и пытается использовать адрес по умолчанию (`localhost:8080`).
  >
  > **Причина:** Переменная окружения `KUBECONFIG`, которая указывает путь к файлу с учетными данными кластера, не была установлена в текущей сессии терминала (например, после перезагрузки компьютера или открытия нового окна терминала).
  >
  > **Решение:** **Перед каждым сеансом работы с `kubectl`** необходимо выполнять команду, устанавливающую эту переменную:
  > ```bash
  > # Выполнять из папки, где лежит kubeconfig.yaml (в нашем случае - terraform/)
  > export KUBECONFIG=$(pwd)/terraform/kubeconfig.yaml
  > ```
  > После этого `kubectl` находит правильные учетные данные и успешно подключается к вашему кластеру в VK Cloud.

</details>

<details>
  <summary><strong>Вопрос: После `kubectl apply -f k8s/` приложение не работает, а `kubectl get pods` показывает, что `postgres` в статусе `Pending`, а `backend` — в `CrashLoopBackOff`.</strong></summary>
  
  > **Ответ:** Это была двухэтапная проблема, где одна ошибка вызывала другую.
  >
  > ```bash
  > vselenaya@computer:~/reminder_app$ kubectl get pods
  > NAME                                   READY   STATUS             RESTARTS       AGE
  > backend-deployment-66745d4956-dpr4l    0/1     CrashLoopBackOff   6 (101s ago)   7m40s
  > frontend-deployment-bc4d4b695-6tm2w    1/1     Running            0              7m40s
  > postgres-deployment-66cb477b74-zkzds   0/1     Pending            0              7m40s
  > ```
  >
  > 1.  **`postgres` в `Pending`**: Статус `Pending` для пода, запрашивающего диск (`PersistentVolumeClaim`), почти всегда означает, что Kubernetes не может выделить ему этот диск. В управляемых кластерах это часто происходит из-за отсутствия или неправильной конфигурации `StorageClass` (провайдера дисков).
  > 2.  **`backend` в `CrashLoopBackOff`**: Этот статус означает, что контейнер запускается, тут же падает с ошибкой, и Kubernetes пытается его перезапустить снова и снова. Просмотр логов (`kubectl logs backend-... --previous`) показал ошибку `connection to server at "postgres-service" ... failed: Connection refused`. `backend` падал, потому что не мог подключиться к базе данных, которая еще даже не запустилась.
  >
  > **Решение:** Для учебного проекта, где сохранность данных не является критичной, было принято решение упростить конфигурацию хранилища для PostgreSQL. В `k8s/database.yaml` использование `PersistentVolumeClaim` (требующее настройки `StorageClass` в облаке) было заменено на **`emptyDir: {}`**. `emptyDir` — это временная директория, которая создается вместе с Подом и живет, пока жив Под. Это не требует настройки дисков в облаке. После применения этого изменения под `postgres` запустился, а `backend` после этого успешно к нему подключился.

</details>

<details>
  <summary><strong>Вопрос: Почему мы масштабируем только `backend`, а не `frontend` или базу данных?</strong></summary>
  
  > **Ответ:** Масштабировать с помощью `HorizontalPodAutoscaler` можно только те компоненты, которые являются **без-состоянийными (stateless)** и **испытывают основную нагрузку**.
  > -   **`backend`**: Идеальный кандидат. Он `stateless` (все данные в БД) и выполняет всю тяжелую работу (бизнес-логику).
  > -   **`frontend`**: Он тоже `stateless`, но его работа очень легкая (отрендерить HTML), и он редко бывает узким местом. Его масштабирование нецелесообразно в данном проекте.
  > -   **`postgres`**: Это `stateful` сервис (с состоянием). Простое масштабирование (`replicas: 2`) привело бы к созданию второй, независимой базы данных и хаосу с данными. Масштабирование баз данных — это отдельная, сложная дисциплина, использующая репликацию или шардирование, и требует специальных объектов Kubernetes, таких как `StatefulSet`.

</details>

<details>
  <summary><strong>Вопрос: Нагрузка на приложение есть, но HPA не увеличивает количество подов.</strong></summary>
  
  > **Ответ:** Это может происходить, если нагрузка недостаточно интенсивна, чтобы превысить установленный порог в 15% CPU. Простой `curl` на главную страницу создает нагрузку в основном на `frontend`.
  > -   **Решение:** Был разработан более интенсивный нагрузочный скрипт, который отправляет несколько одновременных запросов (`curl ... &`) в цикле непосредственно к тем эндпоинтам, которые задействуют `backend` и базу данных (например, `/`). Это заставило CPU бэкенда подняться выше порога и спровоцировало реакцию HPA. В реальных условиях для этого используют специализированные утилиты вроде `wrk` или `JMeter`.

</details>

<details>
  <summary><strong>Вопрос: Ручная установка манифестов Prometheus выдала ошибку `metadata.annotations: Too long...`. Почему выбрали Helm?</strong></summary>
  
  > **Ответ:** Ошибка `Too long` возникает из-за несовместимости версий Kubernetes (v1.28) и манифестов `kube-prometheus`. Разработчики Prometheus вставляют в аннотации очень много текста (включая OpenAPI спецификации для своих кастомных ресурсов), и Kubernetes иногда отказывается принимать YAML-файл, если это поле превышает лимит в 256 КБ.
  > -   **Решение:** Вместо того чтобы вручную редактировать десятки YAML-файлов, было решено использовать **Helm**. Helm — это менеджер пакетов, который использует "чарты" (шаблоны конфигураций). Он умеет правильно генерировать манифесты для конкретной версии кластера, избегая подобных проблем. Установка всего стека мониторинга свелась к одной команде `helm install ...`, что оказалось гораздо проще и надежнее.

</details>

<details>
  <summary><strong>Вопрос: `helm ls -A` показывает `gatekeeper` и `metrics-server`, хотя я их не ставил. Они удалятся при `terraform destroy`?</strong></summary>
  
  > **Ответ:** Да, они удалятся, и вот почему. `gatekeeper` и `metrics-server` являются **аддонами**, которые были установлены в кластер автоматически, как часть процесса его создания, описанного в Terraform. Они не являются частью "ядра" Kubernetes, а скорее дополнительными приложениями, установленными с помощью Helm самим провайдером или Terraform-скриптом.
  >
  > -   **`helm uninstall`**: Удаляет только те релизы, которые вы установили вручную (`prometheus`, `grafana` и т.д.).
  > -   **`kubectl delete`**: Удаляет только те объекты, которые вы создали вручную (`backend-deployment` и т.д.).
  > -   **`terraform destroy`**: Уничтожает **сам кластер** (группу виртуальных машин). Когда кластер удаляется, всё, что было внутри него — и ваше приложение, и эти системные аддоны — исчезает вместе с ним.

</details>

<details>
  <summary><strong>Вопрос: Где Helm хранит свои данные и как не "захламлять" систему?</strong></summary>
  
  > **Ответ:** По умолчанию Helm хранит конфигурацию и кэш в системных папках в домашней директории (`~/.cache/helm/`, `~/.config/helm/`). Чтобы держать всё в одном месте и под контролем, можно использовать переменные окружения.
  >
  > Например, создав папку `.helm_data` внутри проекта, можно запустить Helm так:
  > ```bash
  > XDG_CACHE_HOME=./.helm_data XDG_CONFIG_HOME=./.helm_data XDG_DATA_HOME=./.helm_data helm repo update
  > ```
  > В этом случае все данные Helm будут храниться локально внутри проекта. Для удаления достаточно будет просто удалить папку `.helm_data`.

</details>




### Возможные полезности

- ссылка, где про создание настроек безопасности ssh:
https://cloud.vk.com/docs/tools-for-using-services/terraform/how-to-guides/vnet/secgroups#2224-tabpanel-3

- создание VM:
https://cloud.vk.com/docs/tools-for-using-services/terraform/how-to-guides/iaas/create#4170-tabpanel-0

- ну и быстрый старт:
https://cloud.vk.com/docs/tools-for-using-services/terraform/quick-start#969-tabpanel-1

- отслеживать деньги на аккаунте VK Cloud (подставьте id своего проекта -- например, вида mcs000000000):
https://msk.cloud.vk.com/app/[id]/services/billing/detailing

- скачать kubectl:
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

- Подключение к кластеру через kubectl:
https://cloud.vk.com/docs/kubernetes/k8s/connect/kubectl#10127-tabpanel-1

- создание кластера через terraform:
https://cloud.vk.com/docs/kubernetes/k8s/service-management/create-cluster/create-terraform#5957-tabpanel-0

- команды:

1. Для создания виртуалки в VK Cloud:
```bash
~/Downloads/TERRAFORM/terraform init
~/Downloads/TERRAFORM/terraform apply (вернёт ip виртуальной машины)
~/Downloads/TERRAFORM/terraform destroy (в конце, после окончания работы - чтобы не использовать ресурсы впустую на клауде...)
```

2. Затем регистрируемся в docker hub, создаём и пушим туда наши докер-образы. Заходим на виртуалку и запускаем `sudo docker-compose up -d`
